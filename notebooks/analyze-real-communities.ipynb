{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34fc301c-8ef3-48c4-8bbf-8310efa4e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from data_component import DataComponent\n",
    "from graph_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0116524-6f9c-4a48-8271-51ba62895ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading follow graph ..\n",
      "Reverse edge directionality! \n",
      " BEFORE: u->v: u follows v \n",
      " NOW u<-v: propagation goes from v to u  )\n",
      "Graph loaded  âœ…\n",
      "|V|=7_589 |E|=532_459 node types={'int'}\n"
     ]
    }
   ],
   "source": [
    "# Read graph and opinions\n",
    "data = DataComponent(real_data=\"Brexit\")\n",
    "opinions = data.get_opinions()\n",
    "G_brexit = data.get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04d57b11-64dd-4b12-854a-87d4606b2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of edges removed: 0.28203110474233695\n"
     ]
    }
   ],
   "source": [
    "# Remove edges to obtain a \"reduced\" version of the raw graph (no edges between nodes with opposing stance)\n",
    "edges_to_remove = []\n",
    "gaps = []\n",
    "edges = G_brexit.edges()\n",
    "for edge in edges:\n",
    "    i, j = edge\n",
    "    if i == j:\n",
    "        continue\n",
    "    gaps.append(abs(opinions[i]-opinions[j]))\n",
    "    if (not (opinions[i] >= 0.5 and opinions[j] >= 0.5)) and (not (opinions[i] < 0.5 and opinions[j] < 0.5)):\n",
    "        edges_to_remove.append(edge)\n",
    "        \n",
    "G_brexit_reduced = G_brexit.copy()\n",
    "G_brexit_reduced.remove_edges_from(edges_to_remove)\n",
    "print(\"Proportion of edges removed:\", len(edges_to_remove)/len(edges))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cebefb3-9aed-4740-9025-ed40aef72c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "    \n",
    "\n",
    "def analyze_communities(G, G_raw, opinions, position_type, min_community_size=100):\n",
    "    # Compute the best partition using Louvain method on graph G\n",
    "    partition = community_louvain.best_partition(nx.Graph(G), random_state=42)  \n",
    "    \n",
    "    # Initialize dictionaries to store results\n",
    "    community_nodes_count = defaultdict(int)\n",
    "    community_opinion_avg = {}\n",
    "\n",
    "    # Calculate the number of nodes in each community and their average opinion\n",
    "    for node, comm in partition.items():\n",
    "        community_nodes_count[comm] += 1\n",
    "        if comm not in community_opinion_avg:\n",
    "            community_opinion_avg[comm] = []\n",
    "        community_opinion_avg[comm].append(opinions[node])\n",
    "\n",
    "    # Calculate average opinions and filter communities\n",
    "    large_communities = {comm: nodes for comm, nodes in community_nodes_count.items() if nodes >= min_community_size}\n",
    "    community_avg_opinions = {comm: np.mean(community_opinion_avg[comm]) for comm in large_communities.keys()}\n",
    "\n",
    "    # Identify the community with the highest or lowest average opinion based on metric\n",
    "    target_comm = None\n",
    "    if position_type == \"positive-central\" or position_type == \"positive-high-degree\" or position_type == \"positive-lowest-degree\":\n",
    "        target_comm = max(community_avg_opinions, key=community_avg_opinions.get)\n",
    "    elif position_type == \"negative-central\" or position_type == \"negative-high-degree\" or position_type == \"negative-lowest-degree\":\n",
    "        target_comm = min(community_avg_opinions, key=community_avg_opinions.get)\n",
    "\n",
    "    # Get the nodes in the target community\n",
    "    nodes_in_target_comm = [node for node, community in partition.items() if community == target_comm]\n",
    "\n",
    "    if position_type in [\"positive-central\", \"negative-central\"]:\n",
    "        # Calculate centrality in G_raw\n",
    "        centrality = nx.degree_centrality(G_raw)\n",
    "    elif position_type in [\"positive-high-degree\", \"negative-high-degree\", \"positive-lowest-degree\", \"negative-lowest-degree\"]:\n",
    "        # Calculate degree in G_raw\n",
    "        centrality = dict(G_raw.degree(nodes_in_target_comm))\n",
    "\n",
    "    # Identify the node with the highest or lowest centrality/degree\n",
    "    target_node = None\n",
    "    if position_type == \"positive-lowest-degree\" or position_type == \"negative-lowest-degree\":\n",
    "        target_node = min((node for node in nodes_in_target_comm if centrality[node] > 1), key=lambda node: centrality[node], default=None)\n",
    "    else:\n",
    "        target_node = max(nodes_in_target_comm, key=lambda node: centrality[node])\n",
    "    \n",
    "    return target_node, large_communities, community_avg_opinions, partition\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26809691-56a5-4a8e-a17c-15dca88a6a80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Demo: choose a position type and observe the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c86cf1a9-96cc-42a3-ad59-e913a7fdaf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: positive-central\n",
      "Node: 3638\n",
      "Communities: {0: 3125, 3: 2894, 4: 1343}\n",
      "Avg opinions: {0: 0.8565699737015398, 3: 0.09390009890120354, 4: 0.7998372334670752}\n",
      "Position: negative-central\n",
      "Node: 4426\n",
      "Communities: {0: 3125, 3: 2894, 4: 1343}\n",
      "Avg opinions: {0: 0.8565699737015398, 3: 0.09390009890120354, 4: 0.7998372334670752}\n",
      "Position: positive-high-degree\n",
      "Node: 3638\n",
      "Communities: {0: 3125, 3: 2894, 4: 1343}\n",
      "Avg opinions: {0: 0.8565699737015398, 3: 0.09390009890120354, 4: 0.7998372334670752}\n",
      "Position: negative-high-degree\n",
      "Node: 4426\n",
      "Communities: {0: 3125, 3: 2894, 4: 1343}\n",
      "Avg opinions: {0: 0.8565699737015398, 3: 0.09390009890120354, 4: 0.7998372334670752}\n",
      "Position: positive-lowest-degree\n",
      "Node: 7269\n",
      "Communities: {0: 3125, 3: 2894, 4: 1343}\n",
      "Avg opinions: {0: 0.8565699737015398, 3: 0.09390009890120354, 4: 0.7998372334670752}\n",
      "Position: negative-lowest-degree\n",
      "Node: 14\n",
      "Communities: {0: 3125, 3: 2894, 4: 1343}\n",
      "Avg opinions: {0: 0.8565699737015398, 3: 0.09390009890120354, 4: 0.7998372334670752}\n"
     ]
    }
   ],
   "source": [
    "# Choose:\n",
    "# \"positive-central\": i.e., MOST central in  the most positive community\n",
    "# \"negative-central\": i.e., MOST central in  the most negative community\n",
    "# \"positive-high-degree\": i.e., MAX out-degree in the most positive community\n",
    "# \"negative-high-degree\": i.e., MAX out-degree in the most negative community\n",
    "# \"positive-lowest-degree\": i.e., out-degree > 1\n",
    "# \"negative-lowest-degree\": i.e., out-degree > 1\n",
    "\n",
    "position_llm_node_dict = {}\n",
    "\n",
    "position_types = [\"positive-central\", \"negative-central\", \"positive-high-degree\", \"negative-high-degree\", \"positive-lowest-degree\", \"negative-lowest-degree\"]\n",
    "\n",
    "for position_type in position_types:\n",
    "    nodeid, communities, community_avg_opinions, partition = analyze_communities(G_brexit_reduced, G_brexit, opinions, position_type=position_type)\n",
    "    print(f\"Position: {position_type}\\nNode: {nodeid}\\nCommunities: {communities}\\nAvg opinions: {community_avg_opinions}\")\n",
    "    position_llm_node_dict[position_type] = nodeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13ecfd70-9d36-4b56-85a4-376bc0be0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_dict_path = sys.path[-1] + \"/../data/processed/brexit_position_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "911d0c59-9fc7-45f4-8a72-aa7633f170cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../src/../data/processed/brexit_position_dict.pkl'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_dict_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaa9e566-00c3-4d8d-bf96-37f7a00fa847",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(position_dict_path, \"wb\") as f:\n",
    "    pickle.dump(position_llm_node_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ce7bb7-95e2-41b5-a56d-bd646663785f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4426"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27926800-047b-430a-8105-4bccb7dec398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 3121, 1: 2894, 3: 1337}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6eff565-63f0-47ce-8f24-731a504a1fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.8560817583522307, 1: 0.09390009890120354, 3: 0.799562257202405}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_avg_opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "065da741-cff6-40cd-a4bb-b7e48775c614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition[nodeid] == 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f0032a1-cdcf-4294-9255-20e542d9ae3f",
   "metadata": {},
   "source": [
    "def analyze_communities(G, G_raw, opinions, position_type, min_community_size=100):\n",
    "    # Compute the best partition using Louvain method on graph G\n",
    "    partition = community_louvain.best_partition(nx.Graph(G))\n",
    "    \n",
    "    # Initialize dictionaries to store results\n",
    "    community_nodes_count = defaultdict(int)\n",
    "    community_opinion_avg = {}\n",
    "\n",
    "    # Calculate the number of nodes in each community and their average opinion\n",
    "    for node, comm in partition.items():\n",
    "        community_nodes_count[comm] += 1\n",
    "        if comm not in community_opinion_avg:\n",
    "            community_opinion_avg[comm] = []\n",
    "        community_opinion_avg[comm].append(opinions[node])\n",
    "\n",
    "    # Calculate average opinions and filter communities\n",
    "    large_communities = {comm: nodes for comm, nodes in community_nodes_count.items() if nodes >= min_community_size}\n",
    "    community_avg_opinions = {comm: np.mean(community_opinion_avg[comm]) for comm in large_communities.keys()}\n",
    "\n",
    "    # Identify the community with the highest or lowest average opinion based on metric\n",
    "    target_comm = None\n",
    "    if position_type == \"positive-central\" or position_type == \"positive-high-degree\" or position_type == \"positive-lowest-degree\":\n",
    "        target_comm = max(community_avg_opinions, key=community_avg_opinions.get)\n",
    "    elif position_type == \"negative-central\" or position_type == \"negative-high-degree\" or position_type == \"negative-lowest-degree\":\n",
    "        target_comm = min(community_avg_opinions, key=community_avg_opinions.get)\n",
    "\n",
    "    # Get the nodes in the target community\n",
    "    nodes_in_target_comm = [node for node, community in partition.items() if community == target_comm]\n",
    "\n",
    "    if position_type in [\"positive-central\", \"negative-central\"]:\n",
    "        # Calculate centrality in G_raw\n",
    "        centrality = nx.degree_centrality(G_raw)\n",
    "    elif position_type in [\"positive-high-degree\", \"negative-high-degree\", \"positive-lowest-degree\", \"negative-lowest-degree\"]:\n",
    "        # Calculate degree in G_raw\n",
    "        centrality = dict(G_raw.degree(nodes_in_target_comm))\n",
    "\n",
    "    # Identify the node with the highest or lowest centrality/degree\n",
    "    target_node = None\n",
    "    if position_type == \"positive-central\":\n",
    "        target_node = max(nodes_in_target_comm, key=lambda node: centrality[node])\n",
    "    elif position_type == \"negative-central\":\n",
    "        target_node = max(nodes_in_target_comm, key=lambda node: centrality[node])\n",
    "    elif position_type == \"positive-high-degree\":\n",
    "        target_node = max(nodes_in_target_comm, key=lambda node: centrality[node])\n",
    "    elif position_type == \"negative-high-degree\":\n",
    "        target_node = max(nodes_in_target_comm, key=lambda node: centrality[node])\n",
    "    elif position_type == \"positive-lowest-degree\":\n",
    "        target_node = min((node for node in nodes_in_target_comm if centrality[node] > 1), key=lambda node: centrality[node], default=None)\n",
    "    elif position_type == \"negative-lowest-degree\":\n",
    "        target_node = min((node for node in nodes_in_target_comm if centrality[node] > 1), key=lambda node: centrality[node], default=None)\n",
    "\n",
    "    return target_node, large_communities, community_avg_opinions, partition\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bf5ab70-acaf-48ec-aedb-e885b7398ba8",
   "metadata": {},
   "source": [
    "# BACKLOG: Analyze the obtained communities based on the number of nodes, average and standard dev average opinion, density\n",
    "# !~/anaconda3/envs/socLLM/bin/pip install python-louvain\n",
    "import community as community_louvain\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_communities(G, H, opinions, min_community_size=10):\n",
    "    # Compute the best partition using Louvain method on graph G\n",
    "    partition = community_louvain.best_partition(nx.Graph(G))\n",
    "    \n",
    "    # Initialize dictionaries to store results\n",
    "    community_nodes_count = defaultdict(int)\n",
    "    community_density = {}\n",
    "    community_opinion_avg = {}\n",
    "    community_opinion_std = {}\n",
    "\n",
    "    # Calculate the number of nodes in each community\n",
    "    for node, comm in partition.items():\n",
    "        community_nodes_count[comm] += 1\n",
    "\n",
    "    # Filter out communities with less than the minimum community size\n",
    "    large_communities = {comm: count for comm, count in community_nodes_count.items() if count >= min_community_size}\n",
    "\n",
    "    # Calculate density, average opinion, and std deviation of opinions for each community\n",
    "    for comm in large_communities.keys():\n",
    "        # Nodes in this community\n",
    "        nodes_in_comm = [node for node, community in partition.items() if community == comm]\n",
    "        \n",
    "        # Subgraph induced by nodes in this community in graph H\n",
    "        subgraph = H.subgraph(nodes_in_comm)\n",
    "        \n",
    "        # Density of the community subgraph in H\n",
    "        density = nx.density(subgraph)\n",
    "        community_density[comm] = density\n",
    "        \n",
    "        # Opinions of the nodes in this community\n",
    "        comm_opinions = [opinions[node] for node in nodes_in_comm]\n",
    "        \n",
    "        # Average and std deviation of opinions\n",
    "        avg_opinion = np.mean(comm_opinions)\n",
    "        std_opinion = np.std(comm_opinions)\n",
    "        community_opinion_avg[comm] = avg_opinion\n",
    "        community_opinion_std[comm] = std_opinion\n",
    "\n",
    "    # Return results as dictionaries\n",
    "    return large_communities, community_density, community_opinion_avg, community_opinion_std\n",
    "\n",
    "# Analyze communities\n",
    "nodes_count, density, avg_opinion, std_opinion = analyze_communities(G_brexit_reduced, G_brexit, opinions)\n",
    "\n",
    "# Print results\n",
    "print(\"Community Nodes Count:\", nodes_count)\n",
    "print(\"Community Density:\", density)\n",
    "print(\"Community Average Opinion:\", avg_opinion)\n",
    "print(\"Community Opinion Std Dev:\", std_opinion)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
