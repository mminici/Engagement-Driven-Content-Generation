{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"1,2,3\"\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    Adafactor,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    HfArgumentParser,\n",
    "    pipeline\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The name of the Casual LM model we wish to fine with PPO\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode\n",
    "    # models like gpt-neo* models are more suitable.\n",
    "    model_name: Optional[str] = field(default=\"NousResearch/Llama-2-7b-chat-hf\", metadata={\"help\": \"the model name\"})\n",
    "    tokenizer_name: Optional[str] = field(default=\"\", metadata={\"help\": \"the tokenizer name\"})\n",
    "    reward_model_name: Optional[str] = field(default=\"lvwerra/distilbert-imdb\", metadata={\"help\": \"the reward model name\"})\n",
    "    dataset_name: Optional[str] = field(default=\"\", metadata={\"help\": \"the dataset name\"})\n",
    "    log_with: Optional[str] = field(default=\"wandb\", metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
    "    learning_rate: Optional[float] = field(default=1.41e-5, metadata={\"help\": \"the learning rate\"})\n",
    "    max_length: Optional[int] = field(default=512, metadata={\"help\": \"maximum length for input\"})\n",
    "    output_max_length: Optional[int] = field(default=128, metadata={\"help\": \"maximum length for generation\"})\n",
    "    mini_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the PPO minibatch size\"})\n",
    "    batch_size: Optional[int] = field(default=8, metadata={\"help\": \"the batch size\"})\n",
    "    ppo_epochs: Optional[int] = field(default=1, metadata={\"help\": \"the number of ppo epochs\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=4, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    adafactor: Optional[bool] = field(default=False, metadata={\"help\": \"whether to use the adafactor optimizer\"})\n",
    "    early_stopping: Optional[bool] = field(default=True, metadata={\"help\": \"whether to early stop\"})\n",
    "    target_kl: Optional[float] = field(default=0.1, metadata={\"help\": \"kl target for early stopping\"})\n",
    "    reward_baseline: Optional[float] = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"a baseline value that is subtracted from the reward\"},\n",
    "    )\n",
    "    batched_gen: Optional[bool] = field(default=False, metadata={\"help\": \"whether to use the batched text gen\"})\n",
    "    save_freq: Optional[int] = field(default=None, metadata={\"help\": \"n steps to save the model\"})\n",
    "    output_dir: Optional[str] = field(default=\"/mnt/nas/coppolillo/LLMs/ppo_checkpoints/\",\n",
    "                                      metadata={\"help\": \"n steps to save the model\"})\n",
    "    seed: Optional[int] = field(default=0, metadata={\"help\": \"the seed\"})\n",
    "\n",
    "parser = HfArgumentParser((ScriptArguments,))\n",
    "script_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
    "set_seed(script_args.seed)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Utility functions:\n",
    "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "# from the `datasets` library. One should customize this function to train the model on\n",
    "# its own dataset.\n",
    "def build_dataset(\n",
    "        tokenizer, dataset_name, input_min_text_length=2, input_max_text_length=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    #train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    \n",
    "    prompt = \"Generate a post about cats.\" \n",
    "    \n",
    "    size = 10000\n",
    "    df = pd.DataFrame(np.repeat(prompt, size), columns=[\"0\"])\n",
    "    ds = Dataset.from_dict(df)\n",
    "    \n",
    "    train_dataset = ds.rename_columns({\"0\": \"question\"})\n",
    "    \n",
    "    original_columns = train_dataset.column_names\n",
    "    num_proc = 24\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        new_examples = {\n",
    "            \"query\": [],\n",
    "            \"input_ids\": [],\n",
    "        }\n",
    "        for question in examples[\"question\"]:\n",
    "            query = \"Question: \" + question + \"\\n\\nAnswer: \\n\"\n",
    "            tokenized_question = tokenizer(query, truncation=True, max_length=script_args.max_length)\n",
    "            new_examples[\"query\"].append(query)\n",
    "            new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n",
    "\n",
    "        return new_examples\n",
    "\n",
    "    ds = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=original_columns,\n",
    "    )\n",
    "    ds = ds.filter(lambda x: len(x[\"input_ids\"]) <= script_args.max_length, batched=False)\n",
    "\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Propagation Model\n",
    "import os, sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "\n",
    "from data_component import DataComponent\n",
    "from information_diffusion_component import BoundedConfidenceDiffusionComponent\n",
    "\n",
    "\n",
    "## HYPER PARAMS ------------------------------------\n",
    "# synthetic data generator params\n",
    "num_nodes = 100\n",
    "modularity = 0.5\n",
    "homophily = 0.5\n",
    "\n",
    "# bounded confidence model params\n",
    "epsilon = 0.2\n",
    "mu = 0.5\n",
    "\n",
    "TRAIN = True\n",
    "LOAD = False  # TODO ??\n",
    "output_min_length = 32\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "reward_model_name = script_args.reward_model_name\n",
    "config = PPOConfig(\n",
    "    model_name=script_args.model_name,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    log_with=script_args.log_with,\n",
    "    batch_size=script_args.batch_size,\n",
    "    mini_batch_size=script_args.mini_batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    optimize_cuda_cache=True,\n",
    "    early_stopping=script_args.early_stopping,\n",
    "    target_kl=script_args.target_kl,\n",
    "    ppo_epochs=script_args.ppo_epochs,\n",
    "    seed=script_args.seed\n",
    ")\n",
    "\n",
    "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "rw_kwargs = {\n",
    "    \"return_all_scores\": True,\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": script_args.batch_size,\n",
    "    \"truncation\": True\n",
    "}\n",
    "\n",
    "if \"decapoda\" in script_args.model_name.lower():\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(script_args.model_name)\n",
    "    # required for llama\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "            \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "            \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            \"pad_token\": DEFAULT_PAD_TOKEN,\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)\n",
    "    if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "## GRID EXPERIMENTS\n",
    "modularity_grid = [.2, .8]\n",
    "homophily_grid = [.2, .8]\n",
    "llm_positions = [0, 1, -2, -1]\n",
    "\n",
    "RESULTS = {}\n",
    "\n",
    "for modularity in modularity_grid:\n",
    "    for homophily in homophily_grid:\n",
    "        # 0. Init data component ----------------------------------------\n",
    "        data = DataComponent(num_nodes, modularity, homophily)\n",
    "        data.pre_compute_neighboring()  # save neighbors for each node\n",
    "\n",
    "        # 0.1 Calculate betweenness centrality for all nodes\n",
    "        betweenness_centrality = nx.betweenness_centrality(data.get_graph())\n",
    "        sorted_by_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for position in llm_positions:\n",
    "            # 0.1 Set node position\n",
    "            llm_node_id = sorted_by_betweenness[position]\n",
    "\n",
    "\n",
    "            # 1. Init diffusion model ----------------------------------------\n",
    "            information_diffusion_model = BoundedConfidenceDiffusionComponent(data_component=data, epsilon=epsilon, mu=mu)\n",
    "\n",
    "            opinions = information_diffusion_model.get_opinions()\n",
    "            print(f'Opinions stats \\nmean: {opinions.mean()}\\nstd: {opinions.std()}\\nmin: {opinions.min()}\\nmax: {opinions.max()}')\n",
    "            attrs = {}\n",
    "            for n in data.G.nodes():\n",
    "                attrs.update({n: {\"y\": n, \"x\": [opinions[n]]}})\n",
    "            nx.set_node_attributes(data.G, attrs)\n",
    "            # ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            # 2. Build data component - We retrieve the dataloader by calling the `build_dataset` function.\n",
    "            dataset = build_dataset(tokenizer, script_args.dataset_name)\n",
    "\n",
    "            # ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # 3. Build model components --------------------------------------------\n",
    "            # a. AutoModelForCausalLMWithValueHead, \n",
    "            # b. optimizer,\n",
    "            # c. PPOTrainer\n",
    "            # Now let's build the model, the reference model, and the tokenizer.\n",
    "\n",
    "            lora_config = LoraConfig(\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.05,\n",
    "                bias=\"none\",\n",
    "                task_type=\"CAUSAL_LM\"\n",
    "            )\n",
    "\n",
    "            model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "                config.model_name,\n",
    "                load_in_8bit=False,\n",
    "                device_map=\"auto\",\n",
    "                peft_config=lora_config\n",
    "            )\n",
    "\n",
    "            optimizer = None\n",
    "            if script_args.adafactor:\n",
    "                optimizer = Adafactor(\n",
    "                    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                    scale_parameter=False,\n",
    "                    relative_step=False,\n",
    "                    warmup_init=False,\n",
    "                    lr=config.learning_rate,\n",
    "                )\n",
    "\n",
    "            # We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "            ppo_trainer = PPOTrainer(\n",
    "                config,\n",
    "                model,\n",
    "                ref_model=None,\n",
    "                tokenizer=tokenizer,\n",
    "                dataset=dataset,\n",
    "                data_collator=collator,\n",
    "                optimizer=optimizer\n",
    "            )\n",
    "            device = ppo_trainer.accelerator.device\n",
    "            if ppo_trainer.accelerator.num_processes == 1:\n",
    "                device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a ` pipeline` bug <------- TODO: What is this bug? Why device 0?\n",
    "\n",
    "\n",
    "            # 4. Utility components --------------------------------------------\n",
    "            # 4.1 LengthSampler\n",
    "            output_max_length = script_args.output_max_length\n",
    "            output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "            # 4.2 Sentiment model\n",
    "            sentiment_model = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "            #---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            # 5. Training step --------------------------------------------\n",
    "\n",
    "            if TRAIN:\n",
    "                # We then build the sentiment analysis pipeline, passing the model name and the\n",
    "                # sentiment analysis pipeline arguments. Let's also make sure to set the device\n",
    "                # to the same device as the PPOTrainer.\n",
    "\n",
    "                # reward_model = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "                # We then define the arguments to pass to the `generate` function. These arguments\n",
    "                # are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "                # the `generate` function of the trained model.\n",
    "                generation_kwargs = {\n",
    "                    # \"min_length\": -1,\n",
    "                    \"top_k\": 0.0,\n",
    "                    \"top_p\": 1.0,\n",
    "                    \"do_sample\": True,\n",
    "                    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "                    \"eos_token_id\": 100_000\n",
    "                }\n",
    "            \n",
    "\n",
    "                for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader), total=len(ppo_trainer.dataloader)):\n",
    "                    question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "                    response_tensors = ppo_trainer.generate(\n",
    "                        question_tensors,\n",
    "                        return_prompt=False,\n",
    "                        length_sampler=output_length_sampler,\n",
    "                        **generation_kwargs,\n",
    "                    )\n",
    "                    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "                    # Compute sentiment score\n",
    "                    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "                    sentiment_outputs = sentiment_model(texts, **rw_kwargs)\n",
    "                    # rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in reward_outputs]\n",
    "\n",
    "                    messages_values = [output[1][\"score\"] for output in sentiment_outputs]\n",
    "\n",
    "                    rewards = []\n",
    "                    for message_value in messages_values:\n",
    "                        opinion_shift_tot, num_activated_users, _ = information_diffusion_model.propagate_message(message=message_value,\n",
    "                                                                                                    node_id=llm_node_id)\n",
    "                        rewards.append(torch.tensor(num_activated_users, dtype=torch.float))\n",
    "\n",
    "\n",
    "                    # Run PPO step\n",
    "                    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "                    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "            \n",
    "            #---------------------------------------------------------------------\n",
    "                    \n",
    "\n",
    "            # 6. Save step -------------------------------------------------------\n",
    "            if not os.path.exists(saving_path):\n",
    "                os.makedirs(saving_path)\n",
    "\n",
    "            model.save_pretrained(saving_path)\n",
    "            tokenizer.save_pretrained(saving_path) \n",
    "\n",
    "            saving_path = script_args.output_dir + \"llama2-sentiment-propagation\"\n",
    "            #---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            # 7. Reference model for results -------------------------------------\n",
    "            device_ref_model = \"cuda:0\"\n",
    "            ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(script_args.model_name, device_map=device_ref_model)\n",
    "\n",
    "            if LOAD:\n",
    "                device_model = \"auto\"\n",
    "                        \n",
    "                model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "                    saving_path,\n",
    "                    device_map=device_model\n",
    "                )\n",
    "                \n",
    "                tokenizer = AutoTokenizer.from_pretrained(saving_path)\n",
    "                \n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                \n",
    "                ppo_trainer = PPOTrainer(config, model, None, tokenizer)\n",
    "\n",
    "            assert list(ref_model.parameters())==list(model.parameters())\n",
    "\n",
    "\n",
    "            # 8. Results dataset -------------------------------------------------------\n",
    "            #### get a batch from the dataset\n",
    "\n",
    "            #### 8.1 param setting\n",
    "            bs = 16\n",
    "            game_data = dict()\n",
    "            dataset.set_format(\"pandas\")\n",
    "            df_batch = dataset[:].sample(bs)\n",
    "            game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "            query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "            gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \n",
    "                          \"pad_token_id\": tokenizer.eos_token_id}\n",
    "\n",
    "            \n",
    "\n",
    "            #### 8.2 fill results tensors: ref, trained model\n",
    "            response_tensors_ref = [] \n",
    "            for i in range(bs):\n",
    "                gen_len = output_length_sampler()\n",
    "                output = ref_model.generate(\n",
    "                    torch.tensor(query_tensors[i], device=device_ref_model).unsqueeze(dim=0), max_new_tokens=gen_len, **gen_kwargs\n",
    "                ).squeeze()[-gen_len:]\n",
    "                response_tensors_ref.append(output)\n",
    "\n",
    "            response_tensors = []\n",
    "            for i in range(bs):\n",
    "                output = ppo_trainer.generate(\n",
    "                torch.tensor(query_tensors[i]), max_new_tokens=gen_len, **gen_kwargs\n",
    "                ).squeeze()[-gen_len:]\n",
    "                response_tensors.append(output)\n",
    "\n",
    "            #---------------------------------------------------------------------\n",
    "                \n",
    "            # 9. Results dataset -------------------------------------------------\n",
    "            #### decode responses\n",
    "            game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "            game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "            #### sentiment analysis of query/response pairs before/after\n",
    "            texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "            # game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)] # [output for output in calc_rewards(texts)] # \n",
    "            game_data_sentiments = [output[1][\"score\"] for output in sentiment_model(texts, **rw_kwargs)] # [output for output in calc_rewards(texts)] # \n",
    "            game_data[\"rewards (before)\"] = [information_diffusion_model.propagate_message(message=x,\n",
    "                                            node_id=llm_node_id)[1] for x in game_data_sentiments] # [output for output in calc_rewards(texts)] # \n",
    "\n",
    "\n",
    "            texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "            #game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)] # [output for output in calc_rewards(texts)] # \n",
    "            game_data_sentiments = [output[1][\"score\"] for output in sentiment_model(texts, **rw_kwargs)] # [output for output in calc_rewards(texts)] # \n",
    "            game_data[\"rewards (after)\"] = [information_diffusion_model.propagate_message(message=x,\n",
    "                                            node_id=llm_node_id)[1] for x in game_data_sentiments]\n",
    "\n",
    "            # store results in a dataframe\n",
    "            df_results = pd.DataFrame(game_data)\n",
    "\n",
    "\n",
    "            print(\"mean:\")\n",
    "            display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "            print()\n",
    "            print(\"median:\")\n",
    "            display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())\n",
    "\n",
    "            RESULTS[(modularity, homophily, position, llm_node_id)] = df_results.copy()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c8ff454cd947027f86954d72bf940c689a97dcc494eb53cfe4813862c6065fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}